{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Direct Preference Optimization: Your Language Model is Secretly a Reward Model (DPO)](https://arxiv.org/pdf/2305.18290.pdf)\n",
    "\n",
    "### Reference Code \n",
    "- https://huggingface.co/docs/trl/main/en/dpo_trainer\n",
    "- https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the final dataset object should contain these 3 entries if you use the default DPODataCollatorWithPadding data collator. \n",
    "\n",
    "The entries should be named:\n",
    "- prompt\n",
    "- chosen\n",
    "- rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    HfArgumentParser, \n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from typing import Dict, Optional\n",
    "from trl import DPOTrainer, DPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt(prompt_and_response):\n",
    "    \"\"\"Extract the prompt from a prompt and response pair.\"\"\"\n",
    "    search_term = \"\\nAssistant:\" # Define the search term to locate the Assistant's response\n",
    "    search_term_idx = prompt_and_response.rfind(search_term) # Find the last occurrence of the search term in the text\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\" # Ensure the search term is found\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "def get_rm_static(split: str, sanity_check: bool = False, silent: bool = False, cache_dir: str = None) -> Dataset:\n",
    "    \"\"\"Load the Dahoas/rm-static dataset and preprocess it.\"\"\"\n",
    "    dataset = load_dataset(\"Dahoas/rm-static\", split=split, cache_dir=cache_dir)\n",
    "    if sanity_check:\n",
    "        dataset = dataset.select(range(min(len(dataset), 1000)))\n",
    "\n",
    "    def split_prompt_and_responses(sample) -> Dict[str, str]:\n",
    "        if \"prompt\" not in sample:\n",
    "            raise ValueError(\"Sample does not contain 'prompt' key.\")\n",
    "        prompt = extract_prompt(sample[\"prompt\"])\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": sample[\"chosen\"],\n",
    "            \"rejected\": sample[\"rejected\"],\n",
    "        }\n",
    "\n",
    "    # Apply the transformation\n",
    "    dataset = dataset.map(split_prompt_and_responses)\n",
    "\n",
    "    # Verify the dataset has the correct columns\n",
    "    if not all(col in dataset.column_names for col in [\"prompt\", \"chosen\", \"rejected\"]):\n",
    "        raise ValueError(\"Dataset is missing required columns after transformation.\")\n",
    "\n",
    "    # Remove the 'response' column\n",
    "    if 'response' in dataset.column_names:\n",
    "        dataset = dataset.remove_columns('response')\n",
    "\n",
    "    # Filter out invalid samples\n",
    "    dataset = dataset.filter(lambda x: all(key in x for key in ['prompt', 'chosen', 'rejected']))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check = True\n",
    "train_dataset = get_rm_static(\"train\", sanity_check=sanity_check)\n",
    "eval_dataset = get_rm_static(\"test\", sanity_check=sanity_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset example:\n",
      "{'prompt': '\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:', 'chosen': ' Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it’s lint-free.', 'rejected': ' Yes, you can spray it directly onto the cloth.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset example:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval dataset example:\n",
      "{'prompt': \"\\n\\nHuman: I am trying to write a fairy tale. What is the most popular plot?\\n\\nAssistant: The most popular plot might be “a princess goes to a faraway land, falls in love, and has a magic genie that grants her wishes”.  We can find plenty of examples of this if we search for fairy tales using the search engine Google.\\n\\nHuman: You can't look anything up on google.\\n\\nAssistant: OK, so let’s think about some other popular fairy tales.\\n\\nHuman: Do you like the plot of Hansel and Gretel?\\n\\nAssistant: Not particularly.\\n\\nHuman: what don't you like?\\n\\nAssistant: I feel like the children do very stupid things.\\n\\nHuman: Should I include a witch on my story?\\n\\nAssistant: I think a witch is a fairly common fairy tale element, so yes.  However, what kind of plotline would you like to pursue?\\n\\nHuman: I was thinking I might pursue a feminist plot for a more modern fairytale.\\n\\nAssistant: I would be happy to help you make your story a more feminist one.  Are you writing a new story, or retelling a traditional one?\\n\\nHuman: I think it would be nice to redo a traditional fairytale and make it feminist.\\n\\nAssistant: Do you want me to help you identify parts of the original fairy tale that you would like to modernize?\\n\\nHuman: yes.\\n\\nAssistant: Great! Can you tell me a little bit more about the story that you want to retell?\\n\\nHuman: I think I want to retell snow white.\\n\\nAssistant: Oh, that’s a classic!   Have you seen the Disney movie?  It’s very romantic.   It tells the story of how a beautiful princess gets married to a charming prince.\\n\\nHuman: yes, except I think I want to have the prince character take the place of the snow white character.\\n\\nAssistant: Gotcha.   So, how does this relate to the plot of the traditional fairy tale?\\n\\nHuman: The prince would be stuck in the woods hiding from the witch and waiting for a princess to come for him.\\n\\nAssistant: This would be a really interesting modern retelling.   Would the witch in the new version be a “toxic feminist” or “man hater”?  Or something else?\\n\\nHuman: The witch could be trying to make the prince give up his traditionally female characteristics and turn him into a bad person.\\n\\nAssistant:\", 'chosen': ' This sounds like a really interesting modern retelling of the story!', 'rejected': ' And the prince and the princess both decide that they are more powerful together than apart?'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Eval dataset example:\")\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"gpt2\"\n",
    "ignore_bias_buffers = False\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path) # Load the model from pre-trained weights\n",
    "if ignore_bias_buffers: # Ignore boolean buffers (e.g., bias) during distributed training\n",
    "    # torch distributed hack\n",
    "    model._ddp_params_and_buffers_to_ignore = [\n",
    "        name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "    ]\n",
    "\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(model_name_or_path) # Load a reference copy of the model for comparison\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path) # Load the tokenizer for GPT-2\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Set the padding token to the EOS token if not already defined"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Initialize training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "per_device_train_batch_size = 4 # Reduced batch size for memory efficiency\n",
    "gradient_accumulation_steps = 4 # Gradients accumulate over 4 steps\n",
    "max_length= 512 \n",
    "max_prompt_length = 128 # for input\n",
    "max_target_length =128 # for output\n",
    "label_pad_token_id = 100 # Padding token ID used for labels during training\n",
    "max_steps = 400 # Maximum number of training steps\n",
    "# instrumentation\n",
    "sanity_check = True\n",
    "report_to = None # Specify where to report metrics, if any\n",
    "gradient_checkpointing = False # Whether to use gradient checkpointing to save memory\n",
    "beta = 0.1 # Hyperparameter for DPO (direct preference optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = DPOConfig(\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    max_steps=max_steps,\n",
    "    remove_unused_columns=False, # Keep all columns\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=5,  # match results in blog post; log every 5 steps\n",
    "    eval_steps=500,\n",
    "    output_dir=\"./dpo_model\", # Directory to save the model and outputs\n",
    "    optim=\"rmsprop\",\n",
    "    warmup_steps=150, # Warmup for the first 150 steps\n",
    "    report_to=report_to, # Reporting destination\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    # TODO: uncomment that on the next transformers release\n",
    "    # gradient_checkpointing_kwargs=gradient_checkpointing_kwargs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initialize the DPO trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trl==0.12\n",
    "# !pip install transformers==4.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124846/.local/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_length, max_target_length, max_prompt_length, generate_during_eval. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/jupyter-st124846/.local/lib/python3.12/site-packages/trl/trainer/dpo_trainer.py:397: UserWarning: You passed `generate_during_eval` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/jupyter-st124846/.local/lib/python3.12/site-packages/trl/trainer/dpo_trainer.py:469: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/jupyter-st124846/.local/lib/python3.12/site-packages/trl/trainer/dpo_trainer.py:475: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/jupyter-st124846/.local/lib/python3.12/site-packages/trl/trainer/dpo_trainer.py:481: UserWarning: You passed `max_target_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args = training_args,\n",
    "    beta = beta,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = max_length,\n",
    "    max_target_length = max_target_length,\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    generate_during_eval = True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprapatsorn-along\u001b[0m (\u001b[33mprapatsorn-along-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter-st124846/wandb/run-20250301_155814-jouaqq71</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prapatsorn-along-/huggingface/runs/jouaqq71' target=\"_blank\">./dpo_model</a></strong> to <a href='https://wandb.ai/prapatsorn-along-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prapatsorn-along-/huggingface' target=\"_blank\">https://wandb.ai/prapatsorn-along-/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prapatsorn-along-/huggingface/runs/jouaqq71' target=\"_blank\">https://wandb.ai/prapatsorn-along-/huggingface/runs/jouaqq71</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 07:07, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=3.1107826730229133, metrics={'train_runtime': 430.4389, 'train_samples_per_second': 14.869, 'train_steps_per_second': 0.929, 'total_flos': 0.0, 'train_loss': 3.1107826730229133, 'epoch': 6.4})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Save and push the model to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prapatsorn/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the saved model and tokenizer from checkpoint-400\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./dpo_model/checkpoint-400\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./dpo_model/checkpoint-400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "\n",
    "# # Replace with your token\n",
    "# login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing model and tokenizer to Hugging Face Hub as 'prapatsorn456/dahoas-rm-static-dpo-gpt2'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9458530ae9054b76a0abfbc2a4a83a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3502ecfbbf054049b1e80392d7261c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer successfully pushed to Hugging Face Hub: https://huggingface.co/prapatsorn456/dahoas-rm-static-dpo-gpt2\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face repository information\n",
    "huggingface_username = \"prapatsorn456\" \n",
    "model_name = \"dahoas-rm-static-dpo-gpt2\"  # Model name for the Hugging Face repository\n",
    "repo_name = f\"{huggingface_username}/{model_name}\"\n",
    "\n",
    "# Push the model and tokenizer to Hugging Face Hub\n",
    "print(f\"Pushing model and tokenizer to Hugging Face Hub as '{repo_name}'\")\n",
    "\n",
    "# Make sure the repository is public by setting `private=False`\n",
    "model.push_to_hub(repo_name, private=False)\n",
    "tokenizer.push_to_hub(repo_name, private=False)\n",
    "\n",
    "# Print the link\n",
    "model_url = f\"https://huggingface.co/{repo_name}\"\n",
    "print(f\"Model and tokenizer successfully pushed to Hugging Face Hub: {model_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
